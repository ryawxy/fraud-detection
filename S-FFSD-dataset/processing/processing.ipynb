{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Libraries",
   "id": "d13905edf6c11d1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We begin by importing all the necessary libraries used throughout this notebook.",
   "id": "307e13d614316558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:24.951057Z",
     "start_time": "2025-08-14T04:07:24.947253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.utils import k_hop_subgraph, degree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "import yaml"
   ],
   "id": "5619069e506fef14",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the Dataset",
   "id": "d63c376536811dce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we load the dataset used in our experiment.\n",
    "The dataset is a simulated financial fraud dataset containing the following columns: ` Time `, ` Source `, ` Target `, ` Amount `, ` Location `, ` Type `, and ` Label `. The Label column contains values from 0 to 2, where:\n",
    "\n",
    "- 0 indicates a legitimate transaction,\n",
    "- 1 indicates a fraudulent transaction, and\n",
    "- 2 denotes unlabeled data."
   ],
   "id": "8bd9e1df8e5d6169"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:24.998577Z",
     "start_time": "2025-08-14T04:07:24.962430Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('/S-FFSD-dataset/data/raw/S-FFSD.csv')",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploring the Dataset",
   "id": "40a9bce2d3748ed4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.009376Z",
     "start_time": "2025-08-14T04:07:25.002601Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(10)",
   "id": "2f4bbff1613e422c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Time  Source Target  Amount Location   Type  Labels\n",
       "0     0  S10000  T1000   13.74     L100  TP100       2\n",
       "1     1  S10001  T1001   73.17     L101  TP101       2\n",
       "2     2  S10002  T1000   68.59     L100  TP100       2\n",
       "3     3  S10003  T1002   57.00     L100  TP102       2\n",
       "4     4  S10004  T1000   11.55     L100  TP100       2\n",
       "5     5  S10005  T1000  245.40     L100  TP100       2\n",
       "6     6  S10006  T1000  134.85     L100  TP100       2\n",
       "7     7  S10007  T1000   59.92     L100  TP100       0\n",
       "8     8  S10008  T1003  805.97     L100  TP100       2\n",
       "9     9  S10009  T1000   44.13     L100  TP100       2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Location</th>\n",
       "      <th>Type</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S10000</td>\n",
       "      <td>T1000</td>\n",
       "      <td>13.74</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S10001</td>\n",
       "      <td>T1001</td>\n",
       "      <td>73.17</td>\n",
       "      <td>L101</td>\n",
       "      <td>TP101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S10002</td>\n",
       "      <td>T1000</td>\n",
       "      <td>68.59</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S10003</td>\n",
       "      <td>T1002</td>\n",
       "      <td>57.00</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP102</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S10004</td>\n",
       "      <td>T1000</td>\n",
       "      <td>11.55</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>S10005</td>\n",
       "      <td>T1000</td>\n",
       "      <td>245.40</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>S10006</td>\n",
       "      <td>T1000</td>\n",
       "      <td>134.85</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>S10007</td>\n",
       "      <td>T1000</td>\n",
       "      <td>59.92</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>S10008</td>\n",
       "      <td>T1003</td>\n",
       "      <td>805.97</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>S10009</td>\n",
       "      <td>T1000</td>\n",
       "      <td>44.13</td>\n",
       "      <td>L100</td>\n",
       "      <td>TP100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.044685Z",
     "start_time": "2025-08-14T04:07:25.033602Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "6e74407fe4456442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77881 entries, 0 to 77880\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Time      77881 non-null  int64  \n",
      " 1   Source    77881 non-null  object \n",
      " 2   Target    77881 non-null  object \n",
      " 3   Amount    77881 non-null  float64\n",
      " 4   Location  77881 non-null  object \n",
      " 5   Type      77881 non-null  object \n",
      " 6   Labels    77881 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.068270Z",
     "start_time": "2025-08-14T04:07:25.058879Z"
    }
   },
   "cell_type": "code",
   "source": "df.describe()",
   "id": "362011e9014716f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Time         Amount        Labels\n",
       "count  77881.000000   77881.000000  77881.000000\n",
       "mean   38940.000000     195.624898      1.306249\n",
       "std    22482.452494    4642.508520      0.915825\n",
       "min        0.000000       0.000000      0.000000\n",
       "25%    19470.000000       5.000000      0.000000\n",
       "50%    38940.000000      16.610000      2.000000\n",
       "75%    58410.000000      69.000000      2.000000\n",
       "max    77880.000000  800000.000000      2.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77881.000000</td>\n",
       "      <td>77881.000000</td>\n",
       "      <td>77881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38940.000000</td>\n",
       "      <td>195.624898</td>\n",
       "      <td>1.306249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22482.452494</td>\n",
       "      <td>4642.508520</td>\n",
       "      <td>0.915825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19470.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38940.000000</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58410.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77880.000000</td>\n",
       "      <td>800000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Time-Based Feature Engineering",
   "id": "777d759a6a01d2ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we define a function to perform feature engineering on the ` Time ` column. First, we segment the time values into defined time spans by setting specific upper and lower bounds. This allows us to extract meaningful statistical patterns based on when each transaction occurred.\n",
    "Next, we iterate through the dataset to calculate various statistics within each time span, including the average, total, and standard deviation of transaction amounts, as well as the transaction bias. We also compute the number of transactions, the number of unique locations, and the number of unique transaction types in each span. Finally, we concatenate these newly generated features with the original dataframe."
   ],
   "id": "df8db2c7bf31ffb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.205634Z",
     "start_time": "2025-08-14T04:07:25.202946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def featmap_gen(tmp_df=None):\n",
    "\n",
    "    time_span = [2, 3, 5, 15, 20, 50, 100, 150,\n",
    "                 200, 300, 864, 2590, 5100, 10000, 24000]\n",
    "    time_name = [str(i) for i in time_span]\n",
    "    time_list = tmp_df['Time']\n",
    "    post_fe = []\n",
    "    for trans_idx, trans_feat in tqdm(tmp_df.iterrows()):\n",
    "        new_df = pd.Series(trans_feat)\n",
    "        temp_time = new_df.Time\n",
    "        temp_amt = new_df.Amount\n",
    "        for length, tname in zip(time_span, time_name):\n",
    "            lowbound = (time_list >= temp_time - length)\n",
    "            upbound = (time_list <= temp_time)\n",
    "            correct_data = tmp_df[lowbound & upbound]\n",
    "            new_df['trans_at_avg_{}'.format(\n",
    "                tname)] = correct_data['Amount'].mean()\n",
    "            new_df['trans_at_totl_{}'.format(\n",
    "                tname)] = correct_data['Amount'].sum()\n",
    "            new_df['trans_at_std_{}'.format(\n",
    "                tname)] = correct_data['Amount'].std()\n",
    "            new_df['trans_at_bias_{}'.format(\n",
    "                tname)] = temp_amt - correct_data['Amount'].mean()\n",
    "            new_df['trans_at_num_{}'.format(tname)] = len(correct_data)\n",
    "            new_df['trans_target_num_{}'.format(tname)] = len(\n",
    "                correct_data.Target.unique())\n",
    "            new_df['trans_location_num_{}'.format(tname)] = len(\n",
    "                correct_data.Location.unique())\n",
    "            new_df['trans_type_num_{}'.format(tname)] = len(\n",
    "                correct_data.Type.unique())\n",
    "        post_fe.append(new_df)\n",
    "    return pd.DataFrame(post_fe)"
   ],
   "id": "1e3cffb2a4c133d3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neighbor-Based Feature Engineering",
   "id": "d8963b4417818d4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**In this section, we define three methods for later use.**\n",
    "\n",
    "First, we define a method to find the neighbors of each node in the graph, which will be constructed based on the feature labels of the data points. This helps the model incorporate information from related nodes, enhancing its ability to detect anomalous patterns.\n",
    "\n",
    "The parameter `k` specifies the number of hops for neighbor search. The `where` parameter indicates whether to find incoming or outgoing neighbors. The `choose_risk` label is used to filter the neighbor indices `neigh_idxs`, including only those neighbors whose labels match the specified `risk_label`.\n"
   ],
   "id": "cd6e6d799065a2e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.254336Z",
     "start_time": "2025-08-14T04:07:25.251752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def k_neighs(\n",
    "        graph: Data,\n",
    "        center_idx: int,\n",
    "        k: int,\n",
    "        where: str,\n",
    "        choose_risk: bool = False,\n",
    "        risk_label: int = 1\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if k not in [1, 2]:\n",
    "        raise ValueError(\"k must be 1 or 2\")\n",
    "    flow = 'target_to_source' if where == 'in' else 'source_to_target'\n",
    "\n",
    "    subset, edge_index, mapping, _ = k_hop_subgraph(\n",
    "        center_idx,\n",
    "        num_hops=k,\n",
    "        edge_index=graph.edge_index,\n",
    "        relabel_nodes=True,\n",
    "        flow=flow\n",
    "    )\n",
    "\n",
    "    neigh_idxs = subset[subset != center_idx]\n",
    "\n",
    "    if k == 2:\n",
    "        subset_1hop, _, _, _ = k_hop_subgraph(\n",
    "            center_idx,\n",
    "            num_hops=1,\n",
    "            edge_index=graph.edge_index,\n",
    "            relabel_nodes=True,\n",
    "            flow=flow\n",
    "        )\n",
    "        neigh_1hop = subset_1hop[subset_1hop != center_idx]\n",
    "        neigh_idxs = neigh_idxs[~torch.isin(neigh_idxs, neigh_1hop)]\n",
    "\n",
    "    if choose_risk:\n",
    "        neigh_labels = graph.y[neigh_idxs]\n",
    "        target_idxs = neigh_idxs[neigh_labels == risk_label]\n",
    "    else:\n",
    "        target_idxs = neigh_idxs\n",
    "\n",
    "    return target_idxs"
   ],
   "id": "fe611bc140bc1562",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `count_risk_neighs` method calculates the number of **risky** neighbors each node has in a graph. It iterates through each node, counts how many of its neighbors have a **risky** label, and returns these counts as a tensor.",
   "id": "ef841dd532c9801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.282877Z",
     "start_time": "2025-08-14T04:07:25.280241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_risk_neighs(\n",
    "        graph: Data,\n",
    "        risk_label: int = 1\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    ret = []\n",
    "    for center_idx in range(graph.num_nodes):\n",
    "        neigh_idxs = k_neighs(graph, center_idx, k=1, where=\"out\", choose_risk=True, risk_label=risk_label)\n",
    "        risk_neigh_num = len(neigh_idxs)\n",
    "        ret.append(risk_neigh_num)\n",
    "    return torch.tensor(ret, dtype=torch.float)"
   ],
   "id": "e8c06cb1f57493bc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `feat_map` method is designed to generate node features based on neighborhood information in the graph. It computes aggregated features from the neighbors of each node and returns a tensor containing these features, along with their corresponding feature names. For each node, the method generates the following features:\n",
    "\n",
    "- **1hop_degree**: Sum of the `degree` feature for 1-hop neighbors.\n",
    "- **2hop_degree**: Sum of the `degree` feature for 2-hop neighbors.\n",
    "- **1hop_riskstat**: Sum of the `riskstat` feature for 1-hop neighbors.\n",
    "- **2hop_riskstat**: Sum of the `riskstat` feature for 2-hop neighbors."
   ],
   "id": "dfc3de77d5f4919f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:07:25.318129Z",
     "start_time": "2025-08-14T04:07:25.315879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def feat_map(graph, node_feat):\n",
    "\n",
    "    tensor_list = []\n",
    "    for idx in tqdm(range(graph.num_nodes)):\n",
    "        neighs_1_of_center = k_neighs(graph, idx, 1, \"in\")\n",
    "        neighs_2_of_center = k_neighs(graph, idx, 2, \"in\")\n",
    "\n",
    "        tensor = torch.FloatTensor([\n",
    "            node_feat[neighs_1_of_center, 0].sum().item(),\n",
    "            node_feat[neighs_2_of_center, 0].sum().item(),\n",
    "            node_feat[neighs_1_of_center, 1].sum().item(),\n",
    "            node_feat[neighs_2_of_center, 1].sum().item(),\n",
    "        ])\n",
    "        tensor_list.append(tensor)\n",
    "\n",
    "    feat_names = [\"1hop_degree\", \"2hop_degree\",\n",
    "                  \"1hop_riskstat\", \"2hop_riskstat\"]\n",
    "\n",
    "    tensor_list = torch.stack(tensor_list)\n",
    "    return tensor_list, feat_names"
   ],
   "id": "84e4df5c071c70c6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preprocessing",
   "id": "17ef7d008b893f0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As a first step, we perform feature engineering using the previously defined ` featmap_gen ` function.",
   "id": "a4bb0c9d5926b7d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:30:59.156321Z",
     "start_time": "2025-08-14T04:07:25.325482Z"
    }
   },
   "cell_type": "code",
   "source": "df = featmap_gen(df.reset_index(drop=True))",
   "id": "14a185f8b8cca1a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77881it [23:30, 55.23it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Next, we handle the missing values by filling them with zeros.**\n",
    "\n",
    "This approach is appropriate because the number of missing entries is relatively small compared to the size of the dataset. Moreover, since the dataset is simulated, there is no real-world information available to impute the missing values more accurately."
   ],
   "id": "ef06d8b109ae5d0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:30:59.199677Z",
     "start_time": "2025-08-14T04:30:59.181154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.replace(np.nan, 0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ],
   "id": "9c857b9940f133c8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this part we create an adjeceny matrix for the categorical features.\n",
    "To begin, we initialize three empty lists:\n",
    "\n",
    "- ` out `: Stores the final output results.\n",
    "- ` alls `: Keeps track of the source nodes.\n",
    "- ` allt `: Keeps track of the target nodes.\n",
    "\n",
    "Next, in the **outer loop**, we iterate through each column specified in the `pair` list.\n",
    "Within the **inner loop**, we group the data based on the current column. For each group, we identify transactions that share the same value and create edges between them.\n",
    "\n",
    "However, to limit the number of connections and preserve temporal relevance, we only create edges between transactions that fall within a defined sequential threshold, specified by the `edge_per_trans parameter`."
   ],
   "id": "dcd105361bdac3b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:30:59.267744Z",
     "start_time": "2025-08-14T04:30:59.205539Z"
    }
   },
   "cell_type": "code",
   "source": "#df = pd.read_csv('/S-FFSD-dataset/data/processed/df.csv')",
   "id": "e98aedd717078f20",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/S-FFSD-dataset/data/processed/df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/S-FFSD-dataset/data/processed/df.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/S-FFSD-dataset/data/processed/df.csv'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:25.644256Z",
     "start_time": "2025-08-14T04:31:24.174483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alls = []\n",
    "allt = []\n",
    "pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "edge_per_trans = 3\n",
    "\n",
    "for column in pair:\n",
    "    src, tgt = [], []\n",
    "    for c_id, c_df in tqdm(df.groupby(column), desc=column):\n",
    "        c_df = c_df.sort_values(by=\"Time\")\n",
    "        df_len = len(c_df)\n",
    "        sorted_idxs = c_df.index\n",
    "        src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                    for j in range(edge_per_trans) if i + j < df_len])\n",
    "        tgt.extend([sorted_idxs[i+j] for i in range(df_len)\n",
    "                    for j in range(edge_per_trans) if i + j < df_len])\n",
    "    alls.extend(src)\n",
    "    allt.extend(tgt)"
   ],
   "id": "174288e391c89d62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source: 100%|██████████| 30346/30346 [00:00<00:00, 31953.18it/s]\n",
      "Target: 100%|██████████| 886/886 [00:00<00:00, 6349.44it/s]\n",
      "Location: 100%|██████████| 296/296 [00:00<00:00, 2343.28it/s]\n",
      "Type: 100%|██████████| 166/166 [00:00<00:00, 1304.04it/s]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we store the edge information in `edge_index` tensor.",
   "id": "a49d7fb83202420b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:29.439166Z",
     "start_time": "2025-08-14T04:31:29.356104Z"
    }
   },
   "cell_type": "code",
   "source": "edge_index = torch.tensor([alls, allt], dtype=torch.long)",
   "id": "94332a3ae30bedda",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As part of preprocessing, we need to ensure that all data is numeric, as non-numeric values can cause issues during the training process. Therefore, all categorical columns must be encoded.",
   "id": "49eb2b176dd062a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:31.324718Z",
     "start_time": "2025-08-14T04:31:31.277098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "for col in cal_list:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))"
   ],
   "id": "b523af50210ca1eb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's split the data into feature and target sets.",
   "id": "f243ba9ff5b31bfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:33.294423Z",
     "start_time": "2025-08-14T04:31:33.286762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feat_data = df.drop(\"Labels\", axis=1)\n",
    "labels = df[\"Labels\"]"
   ],
   "id": "1101ad42e831f0bb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to convert the feature nodes and label nodes into tensors to create the graph object in the next step.",
   "id": "e378320c70ec4bdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:34.615751Z",
     "start_time": "2025-08-14T04:31:34.599622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor(feat_data.values, dtype=torch.float32)\n",
    "y = torch.tensor(labels.values, dtype=torch.long)"
   ],
   "id": "876493860139f944",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:31:35.550687Z",
     "start_time": "2025-08-14T04:31:35.547991Z"
    }
   },
   "cell_type": "code",
   "source": "graph = Data(x=x, edge_index=edge_index, y=y)",
   "id": "929fb9ff806d2f03",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now it's time to use the previously defined methods to create neighbor-based features, which include:\n",
    "\n",
    "- The in-degree of each node\n",
    "- The number of risky neighbors for each node\n",
    "\n",
    "Next, we fill any missing values with zero and concatenate these features into a single DataFrame. Finally, to ensure that all feature values are within a similar range, we standardize the DataFrame.\n"
   ],
   "id": "15397cf97a469b25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:44:28.328367Z",
     "start_time": "2025-08-14T04:31:37.465692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"graph info: {graph}\")\n",
    "\n",
    "node_feat = torch.cat([\n",
    "degree(graph.edge_index[1], num_nodes=graph.num_nodes).unsqueeze(1).float(),\n",
    "count_risk_neighs(graph).unsqueeze(1).float()], dim=1)\n",
    "\n",
    "origin_feat_name = ['degree', 'riskstat']\n",
    "\n",
    "features_neigh, feat_names = feat_map(graph, node_feat)\n",
    "features_neigh = torch.cat((node_feat, features_neigh), dim=1).numpy()\n",
    "feat_names = origin_feat_name + feat_names\n",
    "features_neigh[np.isnan(features_neigh)] = 0.\n",
    "features_neigh = pd.DataFrame(features_neigh, columns=feat_names)"
   ],
   "id": "d81c5fe296879e5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph info: Data(x=[77881, 126], edge_index=[2, 860968], y=[77881])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77881/77881 [10:05<00:00, 128.68it/s]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:44:28.528001Z",
     "start_time": "2025-08-14T04:44:28.359248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "features_neigh = pd.DataFrame(scaler.fit_transform(features_neigh), columns=features_neigh.columns)\n",
    "features_neigh.to_csv('/Users/raya/Desktop/fraud-detection/S-FFSD-dataset/data/processed/features_neigh.csv', index=False)"
   ],
   "id": "a4810af503dee39a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entropy-Based Feature Engineering",
   "id": "6305887748081a38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An important feature in fraud detection problems is **Trading Entropy**. For each user, trading entropy can be calculated to capture the variability in their trading behavior. A significant deviation from the normal entropy pattern may indicate a higher probability of fraudulent activity.",
   "id": "34cb6850197e0c76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:44:28.532582Z",
     "start_time": "2025-08-14T04:44:28.530530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calcu_trading_entropy(\n",
    "        data_2: pd.DataFrame\n",
    ") -> float:\n",
    "\n",
    "    if len(data_2) == 0:\n",
    "        return 0\n",
    "\n",
    "    amounts = np.array([data_2[data_2['Type'] == type]['Amount'].sum()\n",
    "                        for type in data_2['Type'].unique()])\n",
    "    proportions = amounts / amounts.sum() if amounts.sum() else np.ones_like(amounts)\n",
    "    ent = -np.array([proportion * np.log(1e-5 + proportion)\n",
    "                     for proportion in proportions]).sum()\n",
    "    return ent"
   ],
   "id": "98b3ffa577286751",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Transformation",
   "id": "65d8be3fdc7444ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:44:28.560576Z",
     "start_time": "2025-08-14T04:44:28.557656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def span_data_2d(\n",
    "        data: pd.DataFrame,\n",
    "        time_windows=None\n",
    ") -> Any:\n",
    "\n",
    "    if time_windows is None:\n",
    "        time_windows = [1, 3, 5, 10, 20, 50, 100, 500]\n",
    "    data = data[data['Labels'] != 2]\n",
    "\n",
    "    nume_feature_ret, label_ret = [], []\n",
    "    for row_idx in tqdm(range(len(data))):\n",
    "        record = data.iloc[row_idx]\n",
    "        acct_no = record['Source']\n",
    "        feature_of_one_record = []\n",
    "\n",
    "        for time_span in time_windows:\n",
    "            feature_of_one_timestamp = []\n",
    "            prev_records = data.iloc[(row_idx - time_span):row_idx, :]\n",
    "            prev_and_now_records = data.iloc[(\n",
    "                                                     row_idx - time_span):row_idx + 1, :]\n",
    "            prev_records = prev_records[prev_records['Source'] == acct_no]\n",
    "\n",
    "            feature_of_one_timestamp.append(\n",
    "                prev_records['Amount'].sum() / time_span)\n",
    "            feature_of_one_timestamp.append(prev_records['Amount'].sum())\n",
    "            feature_of_one_timestamp.append(\n",
    "                record['Amount'] - feature_of_one_timestamp[0])\n",
    "            feature_of_one_timestamp.append(len(prev_records))\n",
    "            old_ent = calcu_trading_entropy(prev_records[['Amount', 'Type']])\n",
    "            new_ent = calcu_trading_entropy(\n",
    "                prev_and_now_records[['Amount', 'Type']])\n",
    "            feature_of_one_timestamp.append(old_ent - new_ent)\n",
    "\n",
    "            feature_of_one_record.append(feature_of_one_timestamp)\n",
    "\n",
    "        nume_feature_ret.append(feature_of_one_record)\n",
    "        label_ret.append(record['Labels'])\n",
    "\n",
    "    nume_feature_ret = np.array(nume_feature_ret).transpose(0, 2, 1)\n",
    "\n",
    "    assert nume_feature_ret.shape == (\n",
    "        len(data), 5, len(time_windows)), \"output shape invalid.\"\n",
    "\n",
    "    return nume_feature_ret.astype(np.float32), np.array(label_ret).astype(np.int64)"
   ],
   "id": "3343adcdf39b00d",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:44:28.575764Z",
     "start_time": "2025-08-14T04:44:28.571649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def span_data_3d(\n",
    "        data: pd.DataFrame,\n",
    "        time_windows=None,\n",
    "        spatio_windows=None,\n",
    ") -> Any:\n",
    "\n",
    "    if time_windows is None:\n",
    "        time_windows = [1, 3, 5, 10, 20, 50, 100, 500]\n",
    "    if spatio_windows is None:\n",
    "        spatio_windows = [1, 2, 3, 4,5]\n",
    "    data = data[data['Labels'] != 2]\n",
    "    data['Location'] = data['Location'].apply(lambda x: int(x.split('L')[1]))\n",
    "    data['Location'] = data['Location'].apply(lambda x: 1 if x == 100 else x)\n",
    "    data['Location'] = data['Location'].apply(lambda x: 2 if 102 >= x > 100 else x)\n",
    "    data['Location'] = data['Location'].apply(lambda x: 3 if 110 >= x > 102 else x)\n",
    "    data['Location'] = data['Location'].apply(lambda x: 4 if 140 >= x > 110 else x)\n",
    "    data['Location'] = data['Location'].apply(lambda x: 5 if x > 140 else x)\n",
    "\n",
    "    nume_feature_ret, label_ret = [], []\n",
    "    for row_idx in tqdm(range(len(data))):\n",
    "        record = data.iloc[row_idx]\n",
    "        acct_no = record['Source']\n",
    "        location = int(record['Location'])\n",
    "        feature_of_one_record = []\n",
    "        for time_span in time_windows:\n",
    "            feature_of_one_timestamp = []\n",
    "            prev_records = data.iloc[(row_idx - time_span):row_idx, :]\n",
    "            prev_and_now_records = data.iloc[(\n",
    "                                                     row_idx - time_span):row_idx + 1, :]\n",
    "            prev_records = prev_records[prev_records['Source'] == acct_no]\n",
    "\n",
    "            for spatio_span in spatio_windows:\n",
    "                feature_of_one_spatio_stamp = []\n",
    "                one_spatio_records = prev_records[prev_records['Location'] > location - spatio_span]\n",
    "                one_spatio_records = one_spatio_records[one_spatio_records['Location'] < location + spatio_span]\n",
    "\n",
    "                feature_of_one_spatio_stamp.append(\n",
    "                    one_spatio_records['Amount'].sum() / time_span)\n",
    "                feature_of_one_spatio_stamp.append(one_spatio_records['Amount'].sum())\n",
    "                feature_of_one_spatio_stamp.append(\n",
    "                    record['Amount'] - feature_of_one_spatio_stamp[0])\n",
    "                feature_of_one_spatio_stamp.append(len(one_spatio_records))\n",
    "\n",
    "                old_ent = calcu_trading_entropy(prev_records[['Amount', 'Type']])\n",
    "                new_ent = calcu_trading_entropy(\n",
    "                    prev_and_now_records[['Amount', 'Type']])\n",
    "                feature_of_one_spatio_stamp.append(old_ent - new_ent)\n",
    "\n",
    "                feature_of_one_timestamp.append(feature_of_one_spatio_stamp)\n",
    "            feature_of_one_record.append(feature_of_one_timestamp)\n",
    "        nume_feature_ret.append(feature_of_one_record)\n",
    "        label_ret.append(record['Labels'])\n",
    "\n",
    "    nume_feature_ret = np.array(nume_feature_ret)\n",
    "    print(nume_feature_ret.shape)\n",
    "    assert nume_feature_ret.shape == (\n",
    "        len(data), len(time_windows), len(spatio_windows), 5), \"output shape invalid.\"\n",
    "\n",
    "    return nume_feature_ret.astype(np.float32), np.array(label_ret).astype(np.int64)"
   ],
   "id": "5f82ec2bbafd0d3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "ab9c93edcce66a5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:51:13.046301Z",
     "start_time": "2025-08-14T04:51:13.044376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_args():\n",
    "    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter,\n",
    "                            conflict_handler='resolve')\n",
    "    parser.add_argument(\"--method\", default=\"mcnn\", type=str)\n",
    "    args = parser.parse_args(args=[])\n",
    "    method = args.method\n",
    "    yaml_file = \"/S-FFSD-dataset/config/cfg.yaml\"\n",
    "    with open(yaml_file) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    config['method'] = method\n",
    "    return config"
   ],
   "id": "40c3405e77e0b8eb",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:51:52.713727Z",
     "start_time": "2025-08-14T04:51:52.711315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def base_load_data(args: dict):\n",
    "    data_path = \"/S-FFSD-dataset/data/raw/S-FFSD.csv\"\n",
    "    feat_df = pd.read_csv(data_path)\n",
    "    train_size = 1 - args['test_size']\n",
    "    features, labels = span_data_2d(feat_df)\n",
    "    trf, tef, trl, tel = train_test_split(\n",
    "        features, labels, train_size=train_size, stratify=labels, shuffle=True)\n",
    "    trf_file, tef_file, trl_file, tel_file = args['trainfeature'], args[\n",
    "        'testfeature'], args['trainlabel'], args['testlabel']\n",
    "    np.save(trf_file, trf)\n",
    "    np.save(tef_file, tef)\n",
    "    np.save(trl_file, trl)\n",
    "    np.save(tel_file, tel)\n",
    "    return"
   ],
   "id": "b3f9a4204da7103e",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:51:54.574012Z",
     "start_time": "2025-08-14T04:51:54.572054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from training import mcnn_main\n",
    "def main(args):\n",
    "        base_load_data(args)\n",
    "        mcnn_main(\n",
    "            args['trainfeature'],\n",
    "            args['trainlabel'],\n",
    "            args['testfeature'],\n",
    "            args['testlabel'],\n",
    "            epochs=args['epochs'],\n",
    "            batch_size=args['batch_size'],\n",
    "            lr=args['lr'],\n",
    "            device=args['device']\n",
    "        )"
   ],
   "id": "30b4585fea2fc418",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:56:09.077665Z",
     "start_time": "2025-08-14T04:51:55.720203Z"
    }
   },
   "cell_type": "code",
   "source": "main(parse_args())",
   "id": "b8e71154942cff4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29643/29643 [03:36<00:00, 137.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.4094, auc: 0.5214, F1: 0.5187, AP: 0.1846\n",
      "Epoch: 1, loss: 0.9019, auc: 0.6538, F1: 0.5826, AP: 0.2513\n",
      "Epoch: 2, loss: 0.7857, auc: 0.6668, F1: 0.5275, AP: 0.2500\n",
      "Epoch: 3, loss: 0.6944, auc: 0.6775, F1: 0.5316, AP: 0.2560\n",
      "Epoch: 4, loss: 0.6663, auc: 0.6808, F1: 0.5513, AP: 0.2602\n",
      "Epoch: 5, loss: 0.6528, auc: 0.7002, F1: 0.5726, AP: 0.2742\n",
      "Epoch: 6, loss: 0.5902, auc: 0.7124, F1: 0.6327, AP: 0.2979\n",
      "Epoch: 7, loss: 0.6682, auc: 0.7054, F1: 0.6646, AP: 0.3110\n",
      "Epoch: 8, loss: 0.5908, auc: 0.7202, F1: 0.6700, AP: 0.3215\n",
      "Epoch: 9, loss: 0.6079, auc: 0.7202, F1: 0.6678, AP: 0.3201\n",
      "Epoch: 10, loss: 0.6548, auc: 0.7050, F1: 0.6732, AP: 0.3171\n",
      "Epoch: 11, loss: 0.5864, auc: 0.7033, F1: 0.6784, AP: 0.3208\n",
      "Epoch: 12, loss: 0.6165, auc: 0.7236, F1: 0.6799, AP: 0.3301\n",
      "Epoch: 13, loss: 0.5937, auc: 0.7189, F1: 0.6816, AP: 0.3294\n",
      "Epoch: 14, loss: 0.6250, auc: 0.7043, F1: 0.6602, AP: 0.3077\n",
      "Epoch: 15, loss: 0.5583, auc: 0.7122, F1: 0.6832, AP: 0.3282\n",
      "Epoch: 16, loss: 0.5655, auc: 0.7292, F1: 0.6767, AP: 0.3303\n",
      "Epoch: 17, loss: 0.5448, auc: 0.7302, F1: 0.6757, AP: 0.3301\n",
      "Epoch: 18, loss: 0.5300, auc: 0.7316, F1: 0.6829, AP: 0.3360\n",
      "Epoch: 19, loss: 0.5496, auc: 0.7361, F1: 0.6754, AP: 0.3328\n",
      "Epoch: 20, loss: 0.5287, auc: 0.7367, F1: 0.6746, AP: 0.3326\n",
      "Epoch: 21, loss: 0.5661, auc: 0.7378, F1: 0.6764, AP: 0.3344\n",
      "Epoch: 22, loss: 0.6268, auc: 0.7350, F1: 0.6642, AP: 0.3254\n",
      "Epoch: 23, loss: 0.5939, auc: 0.7236, F1: 0.6655, AP: 0.3202\n",
      "Epoch: 24, loss: 0.5542, auc: 0.7238, F1: 0.6622, AP: 0.3184\n",
      "Epoch: 25, loss: 0.5364, auc: 0.7193, F1: 0.6650, AP: 0.3179\n",
      "Epoch: 26, loss: 0.5419, auc: 0.7314, F1: 0.6750, AP: 0.3302\n",
      "Epoch: 27, loss: 0.6060, auc: 0.7247, F1: 0.6634, AP: 0.3195\n",
      "Epoch: 28, loss: 0.6721, auc: 0.7030, F1: 0.6709, AP: 0.3146\n",
      "Epoch: 29, loss: 0.5872, auc: 0.7134, F1: 0.6594, AP: 0.3115\n",
      "test set | auc: 0.7050, F1: 0.6867, AP: 0.3291, Recall: 0.7050\n"
     ]
    }
   ],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
